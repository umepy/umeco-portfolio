---
title: "【論文読み】Llama1からLlama2で何が進化したのか？"
date: "2023-07-14T12:00:00+09:00"
tags: ["備忘録", "AI"]
header_image: "/blog_header/girl_study.png"
---

# 今更聞けない大規模言語モデル【Llama2】

## 概要

Llama2 とは 7/26 日に Meta が公開した大規模事前学習済みモデルです。ちなみに読み方はラマです(始めて見たときはエルラマ!?って思ってました)。
Llama2 は公開されているモデルの中では英語においてトップクラスの性能を誇っているそうです。
Llama1 の発表から半年を経たずにアップデートされた Llama2 ですが、何が変わったのか論文を読んでまとめました。

## 大規模言語モデルとは？

大規模言語モデル(LLM: Large Language Model)とは、一言で言うと「大量のテキストデータを学習させた言語モデル」のことで、言語モデルとは文章の次の単語を予測するモデルのことです。次の単語を予測する、という動作を繰り返すことで長い文章を生成することができます。
最近 LLM という言葉をよく聞くようになりましたが、個人的には大きく話題になったのは 2019 年の GPT-2 の発表からだと思っています。この時は OpenAI は GPT-2 のモデルを公開しなかったのですが、その理由が「文章生成の精度が高すぎたため悪用される可能性がある」ということでした。

参考: [当時の記事](https://www.gizmodo.jp/2019/02/elon_musk_backed_text_generating_ai.html)

LLM に関連する論文は近年爆発的に増加している[^1]
![LLMの論文数](/blogs/20230714_llama2/llm_trend.png)

まだこの頃の LLM は単に次の単語を予測するだけのモデルで用途としては文章執筆程度だと思われていて応用は限られていました。しかし、2020 年に入ってからは GPT-3 の発表や、GPT-3 を使った様々な応用が話題になり、LLM の応用範囲が広がってきました。ポイントとしてはスケーリング則と呼ばれる、モデルのパラメータ数/データ量/計算量を増やすことでモデルの精度が急速に向上するということが分かってきたことです。そのため、より大規模なデータセット、より大規模なモデル、より大規模な計算資源を費やすことで、より高い精度のモデルが作れるようになってきました。LLM は次の単語を予測するモデルで学習データは人間が書いた文章であれば何でも良く、それを教師なし学習できるため Web 上の文章を大量に集めることで大規模なデータセットを作ることができます。

[^1]: A Survey of Large Language Models: [URL](https://arxiv.org/abs/2303.18223)

LLM の発展[^1]
![LLM history](/blogs/20230714_llama2/llm_history.png)

そして、ここからより"賢い AI"となるために様々な手法が提案されています。Instruct tuning や RLHF(Reinforcement Learning from Human Feedback)という手法で、Instruct tuning では指示の内容とその返答が正解データとして与えられたデータセットを使って教師あり学習をします。論文としては[FLAN](https://arxiv.org/abs/2109.01652)などで提案されました。また、これだけだとデータの多様性が少なく精度が出ないことが多く、強化学習を使った RLHF という手法も提案されています。モデルに同じ質問に対し複数の回答をしてもらい、その回答に対して点数を付け、いい回答をするように学習させるという手法です。詳しくは[InstructGPT の論文](https://arxiv.org/abs/2203.02155)に説明があります。これらの手法によって学習されたモデルは 10 倍以上のパラメータ数のモデルと同等以上の性能を示すなどの報告がされています。直観的にはこれらの tuning を実施することにより、人間らしい回答をするようになったと感じます(人間が好むような回答をするように tuning したので当たり前ではありますが..)。

### OpenAI のモデルの発展[^1]

![LLM history](/blogs/20230714_llama2/openai.png)

LLM の世界を牽引してきたのは間違いなく OpenAI でしょう。Google や Meta も重要な論文、モデルを発表したりしてきましたが GPT 系列のモデルで話題を作ってきました。

## LLM の課題

このようにして次の単語しか生成できないモデルからより高度な応用ができるモデルへと進化してき、その応用の広さから大きな注目を集めています。ただ、まだまだ課題も多く実社会に応用しようとすると色々難しい点があります。

1. 入出力の長さに制限があること
2. モデルサイズが大きいこと
3. 入出力がテキスト情報に限られること

現時点で最も大きな課題はやはり入出力の制限だと思います。有名な話ですが Transformer の Self-attention では内積を行うので、入力トークン数 x 入力トークン数のサイズの行列が出来ます。つまり入力トークン数の 2 乗の数の浮動小数点数を保存する必要があるのですが、これが多くのメモリを占有してしまうため容易に増やすことが出来ないのです。最近の研究ではこれを削減する手法も様々提案されていますが、基本的には精度とのトレードオフになってしまうのでなかなか解決が出来ていません。

次にモデルサイズですが、高い性能を得るにはモデルサイズは大きくする必要があります。ほとんど全てのモデルでモデルサイズを大きくすればするほど性能は上がっていきます。研究目的であればそれでいいのですが、社会実装する場合はそのサイズが大きくボトルネックとなります。例えば一般消費者が手に入れることが出来る GPU で最も性能が高いのは RTX4090 ですが、この GPU メモリは 24GB です。この時この GPU に乗るモデルは 32bit で読み込むとして 70 億パラメータぐらいです。130 億のモデルになると量子化等をして読み込む必要があります。Llama2 の最大モデルは 700 億パラメータですし、GPT-3 は 1750 億パラメータです。これらのモデルを一般消費者が使うにはまだまだハードルが高いですし、これらを動作させるには大きな計算資源が必要で費用もかかります。

最後は入出力がテキスト情報に限られることです。GPT-4 はマルチモーダル(テキストと画像)で学習されてるとのことなので、全てのモデルが出来ないわけではありません。ただ、現時点では多くのモデルは入出力はテキストだけなので、LLM の応用を考えるときはテキスト情報に限られるということを考慮する必要があります。アイデアとしては画像や動画を含めて色々やりたい、というのはあるのですが、現時点ではまだまだ難しいです。

##

## Llama1 の公開

[^1]: A Survey of Large Language Models: [URL](https://arxiv.org/abs/2303.18223)
